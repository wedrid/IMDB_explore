{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f162ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f95c7644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initDF():\n",
    "    # INIT\n",
    "    dff = pd.read_csv('imdb_dataset.tsv', sep='\\t', header=None, dtype=str)\n",
    "    edges = dff.to_records(index=False)\n",
    "    dff[2] = dff[1].str.extract(r'(\\(\\d{4})', expand=True)#.str.replace(\"(\", \"\").fillna(0).astype(int) #FIXME fillna is pretty ugly rn\n",
    "    dff[2] = dff[2].str.replace(\"(\", \"\", regex=False).fillna(0).astype(int)\n",
    "    dff = dff.rename(columns={0: \"actor\", 1: \"movie\", 2: \"year\"})\n",
    "    dff.loc[dff['movie'] == 'Re']\n",
    "    dff['movie'] = dff['movie'].replace(['Re'],'Re ')\n",
    "    dff['movie'] = dff['movie'].replace(['Regen'], 'Regen ')\n",
    "    return dff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f11d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateIMDBGraph(in_df):\n",
    "    '''\n",
    "    Generates bipartite (0 is actor, 1 is movie) graph of which nodes are \n",
    "    movies and actors, and an edge is present if an actor participated in a movie\n",
    "    Arguments: \n",
    "        in_df: dataframe of which columns are (index, actor, movie)\n",
    "    Returns: \n",
    "        Tuple: \n",
    "            - graph: graph in which nodes are numerical (and not string)\n",
    "            - original_graph: graph in which the nodes are a string\n",
    "            - actor_nodes_f: the list of indices representing the nodes that are of type actor\n",
    "            - movies_nodes_f: the list of indices representing the nodes that are of type movie\n",
    "            - actors_f: list of actors (strings)\n",
    "            - movies_f: list of movies (string)\n",
    "            \n",
    "    '''\n",
    "    in_df = pd.read_csv('imdb_dataset.tsv', sep='\\t', header=None, dtype=str)\n",
    "    edges = in_df.to_records(index=False)\n",
    "    in_df[2] = in_df[1].str.extract(r'(\\(\\d{4})', expand=True)#.str.replace(\"(\", \"\").fillna(0).astype(int) #FIXME fillna is pretty ugly rn\n",
    "    in_df[2] = in_df[2].str.replace(\"(\", \"\", regex=False).fillna(0).astype(int)\n",
    "    in_df = in_df.rename(columns={0: \"actor\", 1: \"movie\", 2: \"year\"})\n",
    "    in_df.loc[in_df['movie'] == 'Re']\n",
    "    in_df['movie'] = in_df['movie'].replace(['Re'],'Re ')\n",
    "    in_df['movie'] = in_df['movie'].replace(['Regen'], 'Regen ')\n",
    "    actors_f = in_df.actor.unique()\n",
    "    movies_f = in_df.movie.unique()\n",
    "    movies_dict = in_df.drop(columns='actor').drop_duplicates().set_index('movie').to_dict('index')\n",
    "    movies_tuples_list = [(k, v) for k, v in movies_dict.items()] #ugly but convenient for what networkx expects\n",
    "    original_graph = nx.Graph()\n",
    "    original_graph.add_nodes_from(actors_f, bipartite = 0) #attribute bipartite following documentation recommendations. In this case 0 is actors_f, 1 is movies_f\n",
    "    print(f\"Number of nodes after adding actors_f is {original_graph.number_of_nodes()}\")\n",
    "    original_graph.add_nodes_from(movies_tuples_list, bipartite = 1)\n",
    "    print(f\"Number of nodes after adding movies_f is {original_graph.number_of_nodes()}\") \n",
    "    original_graph.add_edges_from(edges)\n",
    "    graph = nx.convert_node_labels_to_integers(original_graph, label_attribute='original_name')\n",
    "    actor_nodes_f = {n for n, d in graph.nodes(data=True) if d[\"bipartite\"] == 0}\n",
    "    movies_nodes_f = set(graph) - actor_nodes_f\n",
    "    return (graph, original_graph, actor_nodes_f, movies_nodes_f, actors_f, movies_f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a97e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0655c0b8",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "G) Considering only the movies up to year x with x in {1930,1940,1950,1960,1970,1980,1990,2000,2010,2020}, write a function which, given x, computes the average number of movies per actor up to year x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8075d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgMoviesPerActorUpToYear(graph, act_nodes, mv_nodes, year):\n",
    "    '''\n",
    "    Given 'year' returns the average number of movies in which an actor participated in.\n",
    "    Arguments: \n",
    "        graph: the IMDB graph (numerical version)\n",
    "        act_nodes: indices of nodes which are of type actor (kept in memory so that doesn't have to be calculated each time)\n",
    "        mv_nodes: indices of nodes which are of type movie\n",
    "        year: year up to which calculation is made\n",
    "    Returns: \n",
    "        year: year taken into consideration\n",
    "        mean1: mean taking into consideration the whole set of actor nodes (even unborn and dead ones)\n",
    "        mean2: mean taking into consideration only the actors that made at least one movie in the considered period\n",
    "    '''\n",
    "    # get movies nodes up to a certain year\n",
    "    movies_up_to_year = {x for x,y in graph.nodes(data=True) if y['bipartite'] == 1 and y['year'] <= year}\n",
    "    # \n",
    "    nodes_subset = movies_up_to_year.union(act_nodes) \n",
    "    # We have two ways of interpreting the question. One is to consider actors even when they've zero movies, the \n",
    "    # other is to consider actors only when they have a non zero counter. Regardless, this is considered later\n",
    "    subgraph = graph.subgraph(nodes_subset)\n",
    "    assert subgraph.number_of_nodes() == len(nodes_subset)\n",
    "    \n",
    "    subgraph_actor_nodes = {n for n, d in subgraph.nodes(data=True) if d[\"bipartite\"] == 0} #in this case it's not necessary because actors are first nodes (in order), but what I said is not a given\n",
    "    \n",
    "    degrees = subgraph.degree(nbunch = subgraph_actor_nodes)\n",
    "    deg_data = pd.DataFrame(degrees)\n",
    "    #print(deg_data[1])\n",
    "    sol = (year, deg_data[1].mean(), deg_data[1].replace(0, np.NaN).mean()) #convenient for output later\n",
    "    #print(f\"Mean: {sol[1]}\")\n",
    "    #print(f\"Mean removing zeros: {sol[2]}\")\n",
    "    return sol\n",
    "    \n",
    "    #print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94374f5f",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "3) Considering only the movies up to year x with x in {1930,1940,1950,1960,1970,1980,1990,2000,2010,2020} and restricting to the largest connected component of the graph. Approximate the closeness centrality for each node. Who are the top-10 actors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e285f",
   "metadata": {},
   "source": [
    "[Fast Approximation of Centrality (D. Eppstein, J. Wang)](https://www.ics.uci.edu/~eppstein/pubs/EppWan-SODA-01.pdf)\n",
    "```\n",
    "*Pseudocode\n",
    "1. Let k be the number of iterations needed to obtain the desired error bound\n",
    "2. In iteration i, pick vertex v_i uniformly at random from G and solve the SSSP problem with v_i as a source. \n",
    "3. Let (1) be the centrality estimator for vertex u\n",
    "```\n",
    "Where  \n",
    "(1)  $\\hat{c}_u = \\frac{1}{\\sum_{i=1}^k \\frac{n*d(v_i, u)}{k(n-1)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d81497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def closenessCentralityUpToYear(graph, act_nodes, year, k = None, epsilon = None):\n",
    "    '''\n",
    "    Approximates closeness centrality of graph\n",
    "    Arguments:\n",
    "        graph: input graph (numerical values of nodes)\n",
    "        act_nodes: indices of nodes that are of type actor\n",
    "        year: year up to which the centrality is wanted\n",
    "        [k]: number of iterations, compulsory if epsilon is not specified\n",
    "        [epsilon]: error bound for the centrality estimate (Azuma-Hoeffding epsilon)\n",
    "    Returns:\n",
    "        distances_df: dataframe containing the distances of the sampled nodes, and the calculated centrality\n",
    "        epsilon: epsilon of input (can be None)\n",
    "        k: k of input\n",
    "    \n",
    "    '''\n",
    "    movies_up_to_year = {x for x,y in graph.nodes(data=True) if y['bipartite'] == 1 and y['year'] <= year}\n",
    "    nodes_subset = movies_up_to_year.union(act_nodes) \n",
    "    cc_subgraph = graph.subgraph(nodes_subset)\n",
    "    largest_cc = max(nx.connected_components(cc_subgraph), key=len) # get largest CC \n",
    "    cc_subgraph = graph.subgraph(largest_cc)\n",
    "    assert cc_subgraph.number_of_nodes() == len(largest_cc)\n",
    "    \n",
    "    if epsilon is not None:\n",
    "        import math\n",
    "        k = math.ceil(math.log(cc_subgraph.number_of_nodes())/math.pow(epsilon, 2)) #FIXME: this calculation has to be done on the subgraph\n",
    "        print(f\"Corresponding to epsilon={epsilon} k was calculated as k={k}\")\n",
    "    else: \n",
    "        if k is None: \n",
    "            raise Exception(\"If no epsilon is specified, it is compulsory to specify k (num samples)\")\n",
    "    \n",
    "    print(f\"Value of k is {k} and largest cc size is {len(largest_cc)}\") \n",
    "\n",
    "    # 2. sample k nodes \n",
    "    starting_nodes = random.sample(list(largest_cc), k)\n",
    "    sssp_s = list(map(lambda x: nx.single_source_shortest_path_length(cc_subgraph, x), tqdm(starting_nodes))) # call single_source_shourtest_path_length(cc, sample) for each sample in samples\n",
    "    '''\n",
    "    #I wanted to do a dictionary but we actually don't really care about who generates a sample \n",
    "    sssp_s_dict = {}\n",
    "    for starting_node in tqdm(starting_nodes): \n",
    "        sssp_s_dict[starting_node] = nx.single_source_shortest_path_length(cc_subgraph, starting_node)'''\n",
    "    \n",
    "    n = len(largest_cc)\n",
    "    distances_df = pd.DataFrame(sssp_s).T\n",
    "    distances_df['centrality'] = distances_df.mean(numeric_only=True, axis=1).apply(lambda x: 1/(x*(n/(n-1))))\n",
    "    \n",
    "    \n",
    "    return (distances_df, epsilon, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa7e20ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topNActorsGivenCentralities(centralities_df, N = 10):\n",
    "    '''\n",
    "        Given dataframe with a column \"centrality\" returns the top 10 actors (needed because centrality is calculated for movies, too)\n",
    "        Arguments: \n",
    "            centralities_df: dataframe that has to have a column named 'centrality'\n",
    "            N: lenth of the ranking\n",
    "        Returns:\n",
    "            central_actors: top N actors given centralities\n",
    "    '''\n",
    "    # This method looks at the centralities, and simply excludes movies from the ranking, if present\n",
    "    sorted_centralities = centralities_df.sort_values('centrality', ascending=False)['centrality']\n",
    "    print(type(sorted_centralities))\n",
    "\n",
    "    central_actors = []\n",
    "    i = 0\n",
    "    for index, centr in sorted_centralities.items():\n",
    "        #print(f\"Index {index}, centr: {centr}\")\n",
    "        if G.nodes(data=True)[index]['bipartite'] == 0:\n",
    "            i+=1\n",
    "            central_actors.append(G.nodes(data=True)[index]['original_name'])\n",
    "            if i == 10:\n",
    "                break\n",
    "\n",
    "    return central_actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9330f00f",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "III) Which is the pair of movies that share the largest number of actors?\n",
    "\n",
    "Main idea to solve this would be to do an intersection of the edges of each of the movies.\n",
    "\n",
    "Doing the intersections of all sets can become very expensive timewise. \n",
    "\n",
    "Given an unordered set of sets $\\hat{S} = \\{S_1, .., S_N\\}$ for any $N\\in\\mathbb{N}$ s.t. $|S_i| \\leq M$ for any $i=1,..,N$ and $M \\in\\mathbb{N}$; finding the max intersection would cost $\\mathcal{O}(N)$, and $\\mathcal{O}(\\min{\\{|U_i|,|U_j|\\}})$ (python documentation), reaching $\\mathcal{O}(N*M)$. \n",
    "\n",
    "In this case I use the following simple observations: \n",
    "- For any two given sets $S_i \\neq S_j$ (for $i\\neq j = 1, .., N$) it is true that $|S_i \\cap S_j| \\leq \\min \\{|S_i|, |S_j|\\}$\n",
    "- Let $m$ be the maximum intersection found until a certain iteration. Then if $U_i$ (or $U_j$) is s.t. $|U_i|<m$ (or $|U_j|<m$) then necessarily $|U_i \\cap U_j| < m$, i.e. it is not necessary to do the intersection to infer that the cardinality of that intersection would now surpass the current max. Therefore, it's possible to only check the cardinality and skip the calculation of the intersections.\n",
    "\n",
    "With this heuristic, although the formal complexity would be essentially the same, in practice a lot of the intersections are skipped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1228ad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moviesWithMaxCommonNumActors(graph, mv_nodes):\n",
    "    '''\n",
    "    Calculates the couple of movies with the most common actors\n",
    "    Arguments: \n",
    "        graph: IMDB graph with integer values\n",
    "        mv_nodes: list of indices of the movies that are of type 'movie'\n",
    "    Returns: \n",
    "        (movies1, movie2): solution of the problem\n",
    "    Prints: \n",
    "        Number of common 'actors' between the two movies\n",
    "        \n",
    "    '''\n",
    "    mv_act = nx.to_dict_of_lists(graph)\n",
    "    print(mv_act[1])\n",
    "    current_solution = (None, None)\n",
    "    current_max = 0\n",
    "    for movie in tqdm(mv_nodes):\n",
    "        if len(mv_act[movie]) >= current_max:\n",
    "            for second_movie in mv_nodes:\n",
    "                if len(mv_act[second_movie]) >= current_max and movie != second_movie:\n",
    "                    temp = len(set(mv_act[movie]).intersection(set(mv_act[second_movie])))\n",
    "                    if current_max < temp:\n",
    "                        current_solution = (movie, second_movie)\n",
    "                        current_max = temp\n",
    "                        \n",
    "    print(f\"Max: {current_max}\")\n",
    "    nodes_dt = G.nodes.data(True)\n",
    "    \n",
    "    return (nodes_dt[current_solution[0]]['original_name'], nodes_dt[current_solution[1]]['original_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0192edf",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Build also the actor graph, whose nodes are only actors and two actors are connected if they did a movie together. Answer to the following question:\n",
    "\n",
    "Which is the pair of actors who collaborated the most among themselves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c475facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructGraphAndFindMaxCollaborationGivenActorsGraph(imdb_df_f):\n",
    "    '''\n",
    "    Given IMDB dataset creates actor graph (weighted) and calculates the maximum number of collaborations\n",
    "    Arguments:\n",
    "        imdb_df_f: pandas dataframe of columns (actor, movie) \n",
    "    Returns:\n",
    "        actor_graph: actor graph constructed\n",
    "        mass: the maximum number of collaboration found\n",
    "        sol: tuple of the two actors that collaborated the most\n",
    "    '''\n",
    "    actor_graph_dict = imdb_df_f.groupby('movie')['actor'].apply(list).to_dict()\n",
    "    mass = 0\n",
    "    sol = (None, None)\n",
    "    #k = 0\n",
    "    for movie in tqdm(actor_graph_dict):\n",
    "        current_actors_list = actor_graph_dict[movie]\n",
    "        #if k == 100000:\n",
    "        #    break\n",
    "        #k+=1\n",
    "        for i in range(len(current_actors_list)):\n",
    "            for j in range(i+1, len(current_actors_list)):\n",
    "                if current_actors_list[i] != current_actors_list[j]: \n",
    "                    if not actor_graph.has_edge(current_actors_list[i], current_actors_list[j]):\n",
    "                        actor_graph.add_edge(current_actors_list[i], current_actors_list[j], weight=1)\n",
    "                    else:\n",
    "                        actor_graph[current_actors_list[i]][current_actors_list[j]]['weight'] += 1\n",
    "                        if actor_graph[current_actors_list[i]][current_actors_list[j]]['weight'] > mass:\n",
    "                            mass = actor_graph[current_actors_list[i]][current_actors_list[j]]['weight']\n",
    "                            sol = (current_actors_list[i], current_actors_list[j])\n",
    "    return (actor_graph, mass, sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "917b7f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# even though max is calculated in the creation of the graph, this is how I'd calculate it if the graph was given\n",
    "def findMaxCollaborationGivenGraph(gr):\n",
    "    '''\n",
    "    Finds edge with max weight and returns the two nodes it connects. Also calculate the histogram of the weights. \n",
    "    Arguments: \n",
    "        gr: actor graph\n",
    "    Retruns: \n",
    "        massimo: the found maximal edge\n",
    "        sol: tuple representing the two actors with the most collaborations\n",
    "        hist: a list representing the histogram of the edge weights\n",
    "    '''\n",
    "    archi = gr.edges(data=True)\n",
    "    massimo = 0\n",
    "    sol = (None, None)\n",
    "    hist = [0]*500\n",
    "    i = 0\n",
    "    sol = (None, None)\n",
    "    for arco in archi:\n",
    "        curr_weight = arco[2]['weight']\n",
    "        #print(curr_weight)\n",
    "        if massimo < curr_weight:\n",
    "            massimo =  curr_weight\n",
    "            sol = (arco[0], arco[1])\n",
    "            \n",
    "        hist[curr_weight] += 1\n",
    "        i+=1\n",
    "        if i % 1000000 == 0:\n",
    "            print(f\"Iteration: {i}\")\n",
    "    return (massimo, sol, hist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5720e823",
   "metadata": {},
   "source": [
    "# Function calls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a85667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1413561b7af407892ac182b0a874ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/745941 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000000\n",
      "Iteration: 2000000\n",
      "Iteration: 3000000\n",
      "Iteration: 4000000\n",
      "Iteration: 5000000\n",
      "Iteration: 6000000\n",
      "Iteration: 7000000\n",
      "Iteration: 8000000\n",
      "Iteration: 9000000\n"
     ]
    }
   ],
   "source": [
    "###### MAIN\n",
    "q4 = True\n",
    "# Main for questions 1, 2 and 3\n",
    "if not q4:\n",
    "    df = initDF()\n",
    "    G, oriGinal, actor_nodes, movies_nodes, actors_f, movies_f = generateIMDBGraph(df)\n",
    "    print(\"Test 0\")\n",
    "    ## Q1\n",
    "    \n",
    "    q1_results = []\n",
    "    for year in tqdm(range(1930, 2021, 10)):\n",
    "        q1_results.append(avgMoviesPerActorUpToYear(G, actor_nodes, movies_nodes, year))\n",
    "    q1_df = pd.DataFrame(q1_results).rename(columns={0: \"Year\", 1: \"Average 1\", 2: \"Average 2\"})\n",
    "    q1_df.plot.bar(x=\"Year\", y=[\"Average 1\", \"Average 2\"], title=\"Average number of movies per year\", figsize=(10,8))\n",
    "    \n",
    "    print(\"Test 1\")\n",
    "    ## Q2\n",
    "    data = {}\n",
    "    for year, epsilon in zip(list(range(1930, 2021, 10)), np.linspace(0.05, 0.3, 10)):\n",
    "        data[year] = {}\n",
    "        centralities, eps, kappa = closenessCentralityUpToYear(G, actor_nodes, year, epsilon=epsilon)\n",
    "        data[year]['epsilon'] = epsilon\n",
    "        data[year]['year'] = year\n",
    "        data[year]['centralities'] = centralities\n",
    "        data[year]['num_samples'] = kappa\n",
    "        data[year]['top_ten_actors'] = topNActorsGivenCentralities(centralities, N = 10)\n",
    "        break\n",
    "    \n",
    "    data_tuples = []\n",
    "    cc_sizes = [120720, 180786, 234381, 320719, 451610, 632247, 896126, 1303550, 2380266, 2926072]\n",
    "    it_s = [2.28, 1.36, 1.01, 1.37, 1.90, 2.60, 3.57, 4.85, 8.09, 9.44]\n",
    "    i = 0\n",
    "    for item in data:\n",
    "        record = (data[item]['year'], data[item]['epsilon'], data[item]['num_samples'], cc_sizes[i], it_s[i])\n",
    "        i+=1\n",
    "        data_tuples.append(record)\n",
    "        print(data[item]['top_ten_actors'])\n",
    "\n",
    "    data_df = pd.DataFrame(data_tuples).rename(columns={0: \"Year\", 1: \"Epsilon\", 2: \"Num. Samples\", 3: \"Largest CC size\", 4: \"it/s\"})\n",
    "    print(\"Test 2\")\n",
    "    ## Q3\n",
    "    result = moviesWithMaxCommonNumActors(G, list(movies_nodes)) #2151046\n",
    "    print(result)\n",
    "    print(\"Test 3\")\n",
    "\n",
    "if q4:\n",
    "    ### Q4 # better to be run independently\n",
    "    #df = initDF()\n",
    "    actors = df.actor.unique()\n",
    "    actor_graph = nx.Graph()\n",
    "    actor_graph.add_nodes_from(actors)\n",
    "\n",
    "    actor_graph, maximum, solution = constructGraphAndFindMaxCollaborationGivenActorsGraph(df)\n",
    "    maximum, solution, hist = findMaxCollaborationGivenGraph(actor_graph)\n",
    "    print(\"Test 4\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    figure(figsize=(8, 6))\n",
    "\n",
    "    x = range(0,430)\n",
    "    y = np.array(hist[0:430])\n",
    "    plt.title(\"Distribution of weights (number of collaborations) for each edge\")\n",
    "    plt.scatter(x, y, s=4)\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82efde70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    }
   ],
   "source": [
    "print(maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad831fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apad",
   "language": "python",
   "name": "apad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
